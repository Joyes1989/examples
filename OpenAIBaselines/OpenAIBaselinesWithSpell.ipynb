{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running OpenAI Baselines With Spell (Ex. Training PPO on OpenAI Gym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/spell/baselines'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to /tmp/openai-2019-02-01-22-58-25-575079\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import multiprocessing\n",
    "import os.path as osp\n",
    "import gym\n",
    "from collections import defaultdict\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import spell.metrics as metrics\n",
    "\n",
    "from baselines.common.vec_env.vec_video_recorder import VecVideoRecorder\n",
    "from baselines.common.vec_env.vec_frame_stack import VecFrameStack\n",
    "from baselines.common.cmd_util import common_arg_parser, parse_unknown_args, make_vec_env, make_env\n",
    "from baselines.common.tf_util import get_session\n",
    "from importlib import import_module\n",
    "from baselines.logger import KVWriter, HumanOutputFormat, CSVOutputFormat, JSONOutputFormat, TensorBoardOutputFormat\n",
    "from baselines.ppo2.runner import Runner\n",
    "from baselines.common import explained_variance, set_global_seeds\n",
    "from baselines.common.policies import build_policy\n",
    "from baselines.common.vec_env.vec_normalize import VecNormalize\n",
    "from collections import deque\n",
    "\n",
    "try:\n",
    "    from mpi4py import MPI\n",
    "except ImportError:\n",
    "    MPI = None\n",
    "    \n",
    "try:\n",
    "    import pybullet_envs\n",
    "except ImportError:\n",
    "    pybullet_envs = None\n",
    "\n",
    "try:\n",
    "    import roboschool\n",
    "except ImportError:\n",
    "    roboschool = None\n",
    "    \n",
    "# Load all OpenAI gym environments to dictionary, including Classic Control, Mujoco, Atari, etc.\n",
    "# ----------------------------------------\n",
    "_game_envs = defaultdict(set)\n",
    "for env in gym.envs.registry.all():\n",
    "    env_type = env._entry_point.split(':')[0].split('.')[-1]\n",
    "    _game_envs[env_type].add(env.id)\n",
    "\n",
    "_game_envs['retro'] = {\n",
    "    'BubbleBobble-Nes',\n",
    "    'SuperMarioBros-Nes',\n",
    "    'TwinBee3PokoPokoDaimaou-Nes',\n",
    "    'SpaceHarrier-Nes',\n",
    "    'SonicTheHedgehog-Genesis',\n",
    "    'Vectorman-Genesis',\n",
    "    'FinalFight-Snes',\n",
    "    'SpaceInvaders-Snes',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load OpenAI Baselines Logger Class (modified to send metrics live to Spell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = 10\n",
    "INFO = 20\n",
    "WARN = 30\n",
    "ERROR = 40\n",
    "\n",
    "DISABLED = 50\n",
    "\n",
    "class Logger(object):\n",
    "    DEFAULT = None  # A logger with no output files.\n",
    "    CURRENT = None  # Current logger being used\n",
    "\n",
    "    def __init__(self, dir, output_formats):\n",
    "        self.name2val = defaultdict(float)  # values this iteration\n",
    "        self.name2cnt = defaultdict(int)\n",
    "        self.level = INFO\n",
    "        self.dir = dir\n",
    "        self.output_formats = output_formats\n",
    "\n",
    "    # Logging API,\n",
    "    def logkv(self, key, val):\n",
    "        self.name2val[key] = val\n",
    "        if isinstance(val, np.float32): val = val.item()\n",
    "        print(key, val)\n",
    "        metrics.send_metric(key, val) # Send (key, val) to Spell as metric\n",
    "\n",
    "    def logkv_mean(self, key, val):\n",
    "        if val is None:\n",
    "            self.name2val[key] = None\n",
    "            return\n",
    "        oldval, cnt = self.name2val[key], self.name2cnt[key]\n",
    "        newval = oldval*cnt/(cnt+1) + val/(cnt+1)\n",
    "        self.name2val[key] = newval\n",
    "        if isinstance(newval, np.float32): newval = newval.item()\n",
    "        metrics.send_metric(key, newval) # Send mean (key, val) to Spell as metric\n",
    "        self.name2cnt[key] = cnt + 1\n",
    "\n",
    "    def dumpkvs(self):\n",
    "        if self.level == DISABLED: return\n",
    "        for fmt in self.output_formats:\n",
    "            if isinstance(fmt, KVWriter):\n",
    "                fmt.writekvs(self.name2val)\n",
    "        self.name2val.clear()\n",
    "        self.name2cnt.clear()\n",
    "\n",
    "    def log(self, *args, level=INFO):\n",
    "        if self.level <= level:\n",
    "            self._do_log(args)\n",
    "\n",
    "    # Configuration\n",
    "    # ----------------------------------------\n",
    "    def set_level(self, level):\n",
    "        self.level = level\n",
    "\n",
    "    def get_dir(self):\n",
    "        return self.dir\n",
    "\n",
    "    def close(self):\n",
    "        for fmt in self.output_formats:\n",
    "            fmt.close()\n",
    "\n",
    "    # Misc\n",
    "    # ----------------------------------------\n",
    "    def _do_log(self, args):\n",
    "        for fmt in self.output_formats:\n",
    "            if isinstance(fmt, SeqWriter):\n",
    "                fmt.writeseq(map(str, args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure output log generator with specified format\n",
    "# ----------------------------------------\n",
    "def make_output_format(format, ev_dir, log_suffix=''):\n",
    "    os.makedirs(ev_dir, exist_ok=True)\n",
    "    if format == 'stdout':\n",
    "        return HumanOutputFormat(sys.stdout)\n",
    "    elif format == 'log':\n",
    "        return HumanOutputFormat(osp.join(ev_dir, 'log%s.txt' % log_suffix))\n",
    "    elif format == 'json':\n",
    "        return JSONOutputFormat(osp.join(ev_dir, 'progress%s.json' % log_suffix))\n",
    "    elif format == 'csv':\n",
    "        return CSVOutputFormat(osp.join(ev_dir, 'progress%s.csv' % log_suffix))\n",
    "    elif format == 'tensorboard':\n",
    "        return TensorBoardOutputFormat(osp.join(ev_dir, 'tb%s' % log_suffix))\n",
    "    else:\n",
    "        raise ValueError('Unknown format specified: %s' % (format,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load OpenAI PPO2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spell\n",
    "\n",
    "def constfn(val):\n",
    "    def f(_):\n",
    "        return val\n",
    "    return f\n",
    "\n",
    "def learn(*, network, env, total_timesteps, eval_env = None, seed=None, nsteps=2048, ent_coef=0.0, lr=3e-4,\n",
    "            vf_coef=0.5,  max_grad_norm=0.5, gamma=0.99, lam=0.95,\n",
    "            log_interval=10, nminibatches=4, noptepochs=4, cliprange=0.2,\n",
    "            save_interval=0, load_path=None, model_fn=None, **network_kwargs):\n",
    "    '''\n",
    "    Learn policy using PPO algorithm (https://arxiv.org/abs/1707.06347)\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "\n",
    "    network:                          policy network architecture. Either string (mlp, lstm, lnlstm, cnn_lstm, cnn, cnn_small, conv_only - see baselines.common/models.py for full list)\n",
    "                                      specifying the standard network architecture, or a function that takes tensorflow tensor as input and returns\n",
    "                                      tuple (output_tensor, extra_feed) where output tensor is the last network layer output, extra_feed is None for feed-forward\n",
    "                                      neural nets, and extra_feed is a dictionary describing how to feed state into the network for recurrent neural nets.\n",
    "                                      See common/models.py/lstm for more details on using recurrent nets in policies\n",
    "\n",
    "    env: baselines.common.vec_env.VecEnv     environment. Needs to be vectorized for parallel environment simulation.\n",
    "                                      The environments produced by gym.make can be wrapped using baselines.common.vec_env.DummyVecEnv class.\n",
    "\n",
    "\n",
    "    nsteps: int                       number of steps of the vectorized environment per update (i.e. batch size is nsteps * nenv where\n",
    "                                      nenv is number of environment copies simulated in parallel)\n",
    "\n",
    "    total_timesteps: int              number of timesteps (i.e. number of actions taken in the environment)\n",
    "\n",
    "    ent_coef: float                   policy entropy coefficient in the optimization objective\n",
    "\n",
    "    lr: float or function             learning rate, constant or a schedule function [0,1] -> R+ where 1 is beginning of the\n",
    "                                      training and 0 is the end of the training.\n",
    "\n",
    "    vf_coef: float                    value function loss coefficient in the optimization objective\n",
    "\n",
    "    max_grad_norm: float or None      gradient norm clipping coefficient\n",
    "\n",
    "    gamma: float                      discounting factor\n",
    "\n",
    "    lam: float                        advantage estimation discounting factor (lambda in the paper)\n",
    "\n",
    "    log_interval: int                 number of timesteps between logging events\n",
    "\n",
    "    nminibatches: int                 number of training minibatches per update. For recurrent policies,\n",
    "                                      should be smaller or equal than number of environments run in parallel.\n",
    "\n",
    "    noptepochs: int                   number of training epochs per update\n",
    "\n",
    "    cliprange: float or function      clipping range, constant or schedule function [0,1] -> R+ where 1 is beginning of the training\n",
    "                                      and 0 is the end of the training\n",
    "\n",
    "    save_interval: int                number of timesteps between saving events\n",
    "\n",
    "    load_path: str                    path to load the model from\n",
    "\n",
    "    **network_kwargs:                 keyword arguments to the policy / network builder. See baselines.common/policies.py/build_policy and arguments to a particular type of network\n",
    "                                      For instance, 'mlp' network architecture has arguments num_hidden and num_layers.\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "\n",
    "    set_global_seeds(seed)\n",
    "    type(env)\n",
    "    if isinstance(lr, float): lr = constfn(lr)\n",
    "    else: assert callable(lr)\n",
    "    if isinstance(cliprange, float): cliprange = constfn(cliprange)\n",
    "    else: assert callable(cliprange)\n",
    "    total_timesteps = int(total_timesteps)\n",
    "\n",
    "    policy = build_policy(env, network, **network_kwargs)\n",
    "    # Get the nb of env\n",
    "    nenvs = env.num_envs\n",
    "    # Get state_space and action_space\n",
    "    ob_space = env.observation_space\n",
    "    ac_space = env.action_space\n",
    "    # Calculate the batch_size\n",
    "    nbatch = nenvs * nsteps\n",
    "    nbatch_train = nbatch // nminibatches\n",
    "    \n",
    "    # Instantiate the model object (that creates act_model and train_model)\n",
    "    if model_fn is None:\n",
    "        from baselines.ppo2.model import Model\n",
    "        model_fn = Model\n",
    "    model = model_fn(policy=policy, ob_space=ob_space, ac_space=ac_space, nbatch_act=nenvs, nbatch_train=nbatch_train,\n",
    "                    nsteps=nsteps, ent_coef=ent_coef, vf_coef=vf_coef,\n",
    "                    max_grad_norm=max_grad_norm)\n",
    "    if load_path is not None:\n",
    "        model.load(load_path)\n",
    "        \n",
    "    # Instantiate the runner object\n",
    "    runner = Runner(env=env, model=model, nsteps=nsteps, gamma=gamma, lam=lam)\n",
    "    if eval_env is not None:\n",
    "        eval_runner = Runner(env = eval_env, model = model, nsteps = nsteps, gamma = gamma, lam= lam)\n",
    "    epinfobuf = deque(maxlen=100)\n",
    "    if eval_env is not None:\n",
    "        eval_epinfobuf = deque(maxlen=100)\n",
    "        \n",
    "    # Start total timer\n",
    "    tfirststart = time.time()\n",
    "    nupdates = total_timesteps//nbatch\n",
    "    \n",
    "    for update in range(1, nupdates+1):\n",
    "        assert nbatch % nminibatches == 0      \n",
    "        # Start timer\n",
    "        tstart = time.time()\n",
    "        frac = 1.0 - (update - 1.0) / nupdates      \n",
    "        # Calculate the learning rate\n",
    "        lrnow = lr(frac)     \n",
    "        # Calculate the cliprange\n",
    "        cliprangenow = cliprange(frac)    \n",
    "        # Get minibatch\n",
    "        obs, returns, masks, actions, values, neglogpacs, states, epinfos = runner.run()\n",
    "        if eval_env is not None:\n",
    "            eval_obs, eval_returns, eval_masks, eval_actions, eval_values, eval_neglogpacs, eval_states, eval_epinfos = eval_runner.run() #pylint: disable=E0632\n",
    "        epinfobuf.extend(epinfos)\n",
    "        if eval_env is not None:\n",
    "            eval_epinfobuf.extend(eval_epinfos)\n",
    "            \n",
    "        # Here what we're going to do is for each minibatch calculate the loss and append it.\n",
    "        mblossvals = []\n",
    "        if states is None: # nonrecurrent version\n",
    "            # Index of each element of batch_size\n",
    "            # Create the indices array\n",
    "            inds = np.arange(nbatch)\n",
    "            for _ in range(noptepochs):\n",
    "                # Randomize the indices\n",
    "                np.random.shuffle(inds)\n",
    "                # 0 to batch_size with batch_train_size step\n",
    "                for start in range(0, nbatch, nbatch_train):\n",
    "                    end = start + nbatch_train\n",
    "                    mbinds = inds[start:end]\n",
    "                    slices = (arr[mbinds] for arr in (obs, returns, masks, actions, values, neglogpacs))\n",
    "                    mblossvals.append(model.train(lrnow, cliprangenow, *slices))\n",
    "        else: # recurrent version\n",
    "            assert nenvs % nminibatches == 0\n",
    "            envsperbatch = nenvs // nminibatches\n",
    "            envinds = np.arange(nenvs)\n",
    "            flatinds = np.arange(nenvs * nsteps).reshape(nenvs, nsteps)\n",
    "            envsperbatch = nbatch_train // nsteps\n",
    "            for _ in range(noptepochs):\n",
    "                np.random.shuffle(envinds)\n",
    "                for start in range(0, nenvs, envsperbatch):\n",
    "                    end = start + envsperbatch\n",
    "                    mbenvinds = envinds[start:end]\n",
    "                    mbflatinds = flatinds[mbenvinds].ravel()\n",
    "                    slices = (arr[mbflatinds] for arr in (obs, returns, masks, actions, values, neglogpacs))\n",
    "                    mbstates = states[mbenvinds]\n",
    "                    mblossvals.append(model.train(lrnow, cliprangenow, *slices, mbstates))\n",
    "                    \n",
    "        # Feedforward --> get losses --> update\n",
    "        lossvals = np.mean(mblossvals, axis=0)\n",
    "        # End timer\n",
    "        tnow = time.time()\n",
    "        # Calculate the fps (frame per second)\n",
    "        fps = int(nbatch / (tnow - tstart))\n",
    "        if update % log_interval == 0 or update == 1:\n",
    "            # Calculates if value function is a good predicator of the returns (ev > 1)\n",
    "            # or if it's just worse than predicting nothing (ev =< 0)\n",
    "            ev = explained_variance(values, returns)\n",
    "            # Log (key, value) tuples\n",
    "            Logger.CURRENT.logkv(\"serial_timesteps\", update*nsteps)\n",
    "            Logger.CURRENT.logkv(\"nupdates\", update)\n",
    "            Logger.CURRENT.logkv(\"total_timesteps\", update*nbatch)\n",
    "            Logger.CURRENT.logkv(\"fps\", fps)\n",
    "            Logger.CURRENT.logkv(\"explained_variance\", float(ev))\n",
    "            Logger.CURRENT.logkv('eprewmean', safemean([epinfo['r'] for epinfo in epinfobuf]))\n",
    "            Logger.CURRENT.logkv('eplenmean', safemean([epinfo['l'] for epinfo in epinfobuf]))\n",
    "            if eval_env is not None:\n",
    "                Logger.CURRENT.logger.logkv('eval_eprewmean', safemean([epinfo['r'] for epinfo in eval_epinfobuf]) )\n",
    "                Logger.CURRENT.logger.logkv('eval_eplenmean', safemean([epinfo['l'] for epinfo in eval_epinfobuf]) )\n",
    "            Logger.CURRENT.logkv('time_elapsed', tnow - tfirststart)\n",
    "            for (lossval, lossname) in zip(lossvals, model.loss_names):\n",
    "                Logger.CURRENT.logkv(lossname, lossval)\n",
    "            if MPI is None or MPI.COMM_WORLD.Get_rank() == 0:\n",
    "                Logger.CURRENT.dumpkvs()\n",
    "        if save_interval and (update % save_interval == 0 or update == 1) and Logger.CURRENT.get_dir() and (MPI is None or MPI.COMM_WORLD.Get_rank() == 0):\n",
    "            checkdir = osp.join(Logger.CURRENT.get_dir(), 'checkpoints')\n",
    "            os.makedirs(checkdir, exist_ok=True)\n",
    "            savepath = osp.join(checkdir, '%.5i'%update)\n",
    "            print('Saving to', savepath)\n",
    "            model.save(savepath)\n",
    "    return model\n",
    "\n",
    "# Avoid division error when calculating the mean (in our case if epinfo is empty returns np.nan, not return an error)\n",
    "def safemean(xs):\n",
    "    return np.nan if len(xs) == 0 else np.mean(xs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Arguments and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of training timesteps\n",
    "total_timesteps = 2e7\n",
    "\n",
    "# OpenAI Gym training environment ID (navigate to https://gym.openai.com/envs/ to see the full list of environments)\n",
    "env_id = 'PongNoFrameskip-v4'\n",
    "\n",
    "# Any of {'csv', 'json', 'stdout','tensorboard'}\n",
    "log_formats = ['csv']\n",
    "\n",
    "# Name of folder to which run logs and the trained model will be saved, \n",
    "# accessible via run 'outputs' through the Spell web console\n",
    "save_path = 'pong-ppo' \n",
    "\n",
    "# Dictionary with values of training hyperparameters (see 'learn' fn above for more details)\n",
    "alg_kwargs = {'nsteps': 128} \n",
    "\n",
    "# Policy Network Architecture (for more details, see baselines.common/models.py)\n",
    "network = 'mlp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Training Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve env_type, env_id\n",
    "for env in gym.envs.registry.all():\n",
    "    env_type = env._entry_point.split(':')[0].split('.')[-1]\n",
    "    _game_envs[env_type].add(env.id)  # This is a set so add is idempotent\n",
    "\n",
    "if env_id in _game_envs.keys():\n",
    "    env_type = env_id\n",
    "    env_id = [g for g in _game_envs[env_type]][0]\n",
    "else:\n",
    "    env_type = None\n",
    "    for g, e in _game_envs.items():\n",
    "        if env_id in e:\n",
    "            env_type = g\n",
    "            break\n",
    "            \n",
    "# Build environment\n",
    "ncpu = multiprocessing.cpu_count()\n",
    "if sys.platform == 'darwin': ncpu //= 2\n",
    "nenv = ncpu\n",
    "\n",
    "frame_stack_size = 4\n",
    "env = make_vec_env(env_id, env_type, nenv, None, gamestate=None, reward_scale=1.0)\n",
    "env = VecFrameStack(env, frame_stack_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder where logs and trained model will be saved\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Initialize output formatter\n",
    "output_formats = [make_output_format(f, save_path, '') for f in log_formats]\n",
    "\n",
    "# Create instance of Logger, with appropriate path and output formatter\n",
    "Logger.CURRENT = Logger(dir=save_path, output_formats=output_formats) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'network' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c67754c44a36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run PPO learning function, with parameters set above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0malg_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'network' is not defined"
     ]
    }
   ],
   "source": [
    "# Run PPO learning function, with parameters set above\n",
    "model = learn(network=network, env=env, seed=None, total_timesteps=total_timesteps, **alg_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After learning ends, save trained agent\n",
    "model.save(save_path +'/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To see your trained agent in action, navigate to the Spell Web Console and download the saved model from run outputs. Then, run the command below from your local machine (with the env_id set above and the model file's local path passed in as arguments):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "python -m baselines.run --alg=ppo2 --env=ENV_ID --num_timesteps=0 --load_path=LOCAL_MODEL_PATH --play\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spell - baselines on K80",
   "language": "",
   "name": "spell_632382a3"
  },
  "language_info": {
   "codemirror_mode": "python",
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
